{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates the BTC sentiment analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from btc_sentiment.pipelines.ingest_pipeline import run_ingest_pipeline\n",
    "from btc_sentiment.utils.io import load_daily_sentiment\n",
    "from btc_sentiment.utils.viz import plot_daily_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /workspaces/sentiments-analysis/.venv/lib/python3.11/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error fetching from Market Mercenaries: Cannot find any entity corresponding to \"Market Mercenaries\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m      3\u001b[39m nest_asyncio.apply()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m daily_sentiment = \u001b[43mrun_ingest_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdays_back\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtweets_per_day\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages_per_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(daily_sentiment)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m daily records\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sentiments-analysis/src/btc_sentiment/pipelines/ingest_pipeline.py:51\u001b[39m, in \u001b[36mrun_ingest_pipeline\u001b[39m\u001b[34m(days_back, tweets_per_day, messages_per_channel, output_path)\u001b[39m\n\u001b[32m     48\u001b[39m         r[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] = a.label\n\u001b[32m     50\u001b[39m all_records = tweet_records + msg_records\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m daily: List[DailySentiment] = \u001b[43mAggregator\u001b[49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m save_records(daily, output_path)\n\u001b[32m     53\u001b[39m plot_daily_sentiment(daily)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sentiments-analysis/src/btc_sentiment/services/aggregator.py:17\u001b[39m, in \u001b[36mAggregator.aggregate\u001b[39m\u001b[34m(records)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maggregate\u001b[39m(records: List[Dict]) -> List[DailySentiment]:\n\u001b[32m     16\u001b[39m     df = pd.DataFrame(records)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m).dt.date\n\u001b[32m     18\u001b[39m     results = []\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (source, date), grp \u001b[38;5;129;01min\u001b[39;00m df.groupby([\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sentiments-analysis/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/sentiments-analysis/.venv/lib/python3.11/site-packages/pandas/core/indexes/range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'date'"
     ]
    }
   ],
   "source": [
    "%pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "daily_sentiment = run_ingest_pipeline(\n",
    "    days_back=7,\n",
    "    tweets_per_day=0,\n",
    "    messages_per_channel=100\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(daily_sentiment)} daily records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found. Run the pipeline first.\n"
     ]
    }
   ],
   "source": [
    "data = load_daily_sentiment()\n",
    "\n",
    "if data:\n",
    "    plot_daily_sentiment(data)\n",
    "else:\n",
    "    print(\"No data found. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = load_daily_sentiment()\n",
    "if data:\n",
    "    df = pd.DataFrame([r.dict() for r in data])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    print(f\"Loaded {len(df)} daily sentiment records\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Sentiment score distribution\n",
    "    for source in df['source'].unique():\n",
    "        source_data = df[df['source'] == source]\n",
    "        axes[0,0].hist(source_data['avg_score'], alpha=0.7, label=source, bins=20)\n",
    "    axes[0,0].set_title('Sentiment Score Distribution')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Message count by source\n",
    "    source_counts = df.groupby('source')['count'].sum()\n",
    "    axes[0,1].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')\n",
    "    axes[0,1].set_title('Message Volume by Source')\n",
    "    \n",
    "    # Sentiment labels\n",
    "    label_counts = df['label'].value_counts()\n",
    "    colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n",
    "    bar_colors = [colors.get(label, 'blue') for label in label_counts.index]\n",
    "    axes[1,0].bar(label_counts.index, label_counts.values, color=bar_colors)\n",
    "    axes[1,0].set_title('Sentiment Label Distribution')\n",
    "    \n",
    "    # Daily trend\n",
    "    for source in df['source'].unique():\n",
    "        source_data = df[df['source'] == source].sort_values('date')\n",
    "        axes[1,1].plot(source_data['date'], source_data['avg_score'], marker='o', label=source)\n",
    "    axes[1,1].set_title('Daily Sentiment Trend')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
